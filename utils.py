import torch
import torch.nn.functional as F

def coref_loss(antecedent_scores, gold_antecedents):

    batch_size, num_mentions, max_antecedents = antecedent_scores.shape
    null_scores = torch.zeros(batch_size, num_mentions, 1, device=antecedent_scores.device)
    scores_with_null = torch.cat([null_scores, antecedent_scores], dim=-1)  # (B, M, max_antecedents+1)

    gold_antecedents_clamped = gold_antecedents.clone()
    gold_antecedents_clamped[gold_antecedents_clamped == -1] = 0

    log_probs = F.log_softmax(scores_with_null, dim=-1)  # (B, M, max_antecedents+1)
    gold_log_probs = torch.gather(log_probs, 2, gold_antecedents_clamped.unsqueeze(-1)).squeeze(-1)  # (B, M)
    loss = -gold_log_probs.mean()

    return loss


# def get_gold_antecedents(topk_indices, mention_to_cluster):
#     """
#     topk_indices: List[int] ‚Äî –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–ø-K –º–µ–Ω—à–∏–æ–Ω–æ–≤
#     mention_to_cluster: List[int] ‚Äî —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤—Å–µ—Ö –º–µ–Ω—à–∏–æ–Ω–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
#     –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç List[int] ‚Äî –∏–Ω–¥–µ–∫—Å –∞–Ω—Ç–µ—Ü–µ–¥–µ–Ω—Ç–∞ –∏–∑ topk_indices –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ–Ω—à–∏–æ–Ω–∞, –∏–ª–∏ -1 –µ—Å–ª–∏ –Ω–µ—Ç
#     """
#     gold_antecedents = []
#
#     for i, idx_i in enumerate(topk_indices):
#         cluster_i = mention_to_cluster[idx_i]
#         found = False
#         for j in range(i - 1, -1, -1):
#             idx_j = topk_indices[j]
#             if mention_to_cluster[idx_j] == cluster_i:
#                 gold_antecedents.append(j)
#                 found = True
#                 break
#         if not found:
#             gold_antecedents.append(-1)
#
#     return gold_antecedents

def get_gold_antecedents(topk_indices, mention_to_cluster, all_mentions, input_ids, tokenizer):
    gold_antecedents = []

    for i, idx_i in enumerate(topk_indices):
        cluster_i = mention_to_cluster[idx_i]
        found = False
        for j in range(i - 1, -1, -1):
            idx_j = topk_indices[j]
            if mention_to_cluster[idx_j] == cluster_i:
                gold_antecedents.append(j)
                found = True

                # üü¢ –õ–æ–≥–≥–∏—Ä—É–µ–º –ø–∞—Ä—ã coreferent mentions
                span_i = all_mentions[idx_i]
                span_j = all_mentions[idx_j]
                text_i = tokenizer.decode(input_ids[span_i[0]:span_i[1]+1])
                text_j = tokenizer.decode(input_ids[span_j[0]:span_j[1]+1])
                # print(f"[COREF] {text_j}  ‚Üê antecedent of  {text_i}")
                break

        if not found:
            gold_antecedents.append(-1)
            span_i = all_mentions[idx_i]
            text_i = tokenizer.decode(input_ids[span_i[0]:span_i[1]+1])
            # print(f"[NO COREF] {text_i}  ‚Üê no antecedent")

    return gold_antecedents

# –¥–ª—è –∫–∞–∂–¥–æ–≥–æ mention [i] - –±–µ—Ä–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∞–Ω—Ç–µ—Ü–µ–¥–µ–Ω—Ç gold_ante
# –µ—Å–ª–∏ gold_ante == -1, –∑–Ω–∞—á–∏—Ç –µ–≥–æ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ mention –Ω–µ—Ç - —Å–∫–∏–ø–∞–µ–º
# –±–µ—Ä—ë–º —Å–∫–æ—Ä—ã –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤-–∞–Ω—Ç–µ—Ü–µ–¥–µ–Ω—Ç–æ–≤ antecedent_scores[b, i]
# –∫–æ–Ω–∫–∞—Ç–∏–º —Å–ª–µ–≤–∞ —Å–∫–∞–ª—è—Ä 0.0
# —Å–º–µ—â–∞–µ–º –∏–Ω–¥–µ–∫—Å –∞–Ω—Ç–µ—Ü–µ–¥–µ–Ω—Ç–∞
# –≤–æ–∑–≤—Ä–∞—â–∞–µ–º (-1) * —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –ª–æ–≥ —Å–æ—Ñ—Ç–º–∞–∫—Å –¥–ª—è –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ scores


# FAQ:
# Mention - —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å—É—â–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ (—Ñ—Ä–∞–∑–∞, —Å–ª–æ–≤–æ, –≥—Ä—É–ø–ø–∞)
# –ê–Ω—Ç–µ—Ü–µ–¥–µ–Ω—Ç - –ø—Ä–µ–¥—ã–¥—É—â–∏–π mention –∫ –∫–æ—Ç–æ—Ä–æ–º—É —Ç–µ–∫—É—â–∏–π mention —Å—Å—ã–ª–∞–µ—Ç—Å—è
# –ó–∞–¥–∞—á–∞ - —Å–≤—è–∑–∞—Ç—å –≤—Å–µ mention –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –æ–¥–Ω–æ–º—É –æ–±—ä–µ–∫—Ç—É –≤ –æ–¥–∏–Ω –∫–æ—Ä–µ—Ñ–µ—Ä–µ–Ω—Ç–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä
# –í—Ä–æ–¥–µ –≤–æ—Ç —Ç–∞–∫ —è –∑–∞–¥–∞—á—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–ª —Ö–∑